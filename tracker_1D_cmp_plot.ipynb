{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import operator\n",
    "import pathlib\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "font = {'family': 'Times New Roman', 'weight': 'bold', 'size': 12}\n",
    "rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_metric_results_for_iters(\n",
    "    results_dir_path,\n",
    "    metrics = ('mota', 'motp'),\n",
    "    results_file_patt = 'eval_results.csv',\n",
    "):\n",
    "    metric_vals = collections.defaultdict(list)\n",
    "\n",
    "    for file in pathlib.Path(results_dir_path).rglob(results_file_patt):\n",
    "        model_dir = file.parent.parent\n",
    "        n_model_iters = int(model_dir.stem)\n",
    "        \n",
    "        results = pd.read_csv(str(file), index_col=0)\n",
    "        overall = results.loc['OVERALL']\n",
    "        for metric in metrics:\n",
    "            metric_vals[metric].append((n_model_iters, overall[metric]))\n",
    "    \n",
    "    return metric_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_eval_results_multiple(dir_path_label_pairs, metric, metric_label):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "    \n",
    "    ax.set_title('Comparison Plot of Overall Tracker Metrics')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel(metric_label)\n",
    "\n",
    "    for results_dir_path, label in dir_path_label_pairs:\n",
    "        metric_vals = collect_metric_results_for_iters(results_dir_path, (metric,))\n",
    "        vals = metric_vals[metric]\n",
    "        vals = sorted(vals, key=operator.itemgetter(0))\n",
    "        xs = np.asarray([pair[0] for pair in vals])\n",
    "        ys = np.asarray([pair[1] for pair in vals])\n",
    "        ax.plot(xs, ys, label=label)\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "dir_path_label_pairs = (\n",
    "    ('./eval_orig_mini', 'Original (mini)'), ('./eval_dsa', 'Original')\n",
    ")\n",
    "\n",
    "fig_mota = plot_1d_eval_results_multiple(dir_path_label_pairs, 'mota', 'MOTA')\n",
    "fig_mota.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_motp = plot_1d_eval_results_multiple(dir_path_label_pairs, 'motp', 'MOTP')\n",
    "fig_motp.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76ca744f2f2bbdc513f12385c5c1408b7d19822a54ce48d4d56c79d4676f5a55"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('ml': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

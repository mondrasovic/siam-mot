{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "rc('text', usetex=True)\n",
    "font = {'family': 'Times New Roman', 'weight': 'bold', 'size': 14}\n",
    "rc('font', **font)\n",
    "rcParams['text.latex.preamble'] = [r'\\usepackage{sfmath} \\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vals = []\n",
    "attention_vals = []\n",
    "overall_scores_rows = []\n",
    "columns = None\n",
    "\n",
    "def iter_results_files(*dir_names):\n",
    "    for dir_name in dir_names:\n",
    "        yield from pathlib.Path(dir_name).rglob('eval_results.csv')\n",
    "\n",
    "for results_file in iter_results_files('eval_orig_mini', 'eval_dsa'):\n",
    "    model_dir = results_file.parent.parent\n",
    "    attention_dir = model_dir.parent\n",
    "\n",
    "    model = model_dir.stem\n",
    "    attention = attention_dir.stem\n",
    "\n",
    "    model_vals.append(model)\n",
    "    attention_vals.append(attention)\n",
    "\n",
    "    df = pd.read_csv(str(results_file), index_col=0)\n",
    "    columns = df.columns\n",
    "    overall_scores = df.iloc[-1]\n",
    "    overall_scores_rows.append(overall_scores)\n",
    "\n",
    "df_orig = pd.DataFrame(overall_scores_rows, columns=columns)\n",
    "df_orig.insert(0, 'model', model_vals)\n",
    "df_orig.insert(1, 'attention', attention_vals)\n",
    "df_orig.reset_index(drop=True, inplace=True)\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig[\n",
    "    (df_orig['model'] == '0035000') |\n",
    "    (df_orig['model'] == '0025000') |\n",
    "    (df_orig['model'] == '0055000') |\n",
    "    (df_orig['model'] == '0050000') |\n",
    "    (df_orig['model'] == '0045000') |\n",
    "    (df_orig['model'] == '0040000')\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tracker_attention_comparison(\n",
    "    df, x_col, y_col, x_label, y_label, x_units='\\%', y_units='\\%'\n",
    "):\n",
    "    def _build_axis_label(text, units=None):\n",
    "        label = rf'$\\textbf{{{text}}}$'\n",
    "        if units is not None:\n",
    "            label += f' [{units}]'\n",
    "        return label\n",
    "    \n",
    "    def _get_attention_text(attention):\n",
    "        return {'without_dsa': 'None', 'with_dsa': 'DSA'}[attention]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8), nrows=1, ncols=1)\n",
    "\n",
    "    base_colors =  list(mcolors.BASE_COLORS.keys())\n",
    "    unique_models = df['model'].unique().tolist()\n",
    "\n",
    "    size = 30 ** 2\n",
    "    for attention, group_attention_df in df.groupby(['attention']):\n",
    "        marker = 'o' if 'without' in attention else 'D'\n",
    "        \n",
    "        for model, group_model_df in group_attention_df.groupby(['model']):\n",
    "            xs, ys = group_model_df[x_col], group_model_df[y_col]    \n",
    "            \n",
    "            curr_color = base_colors[unique_models.index(model)]\n",
    "            attention_text = _get_attention_text(attention)\n",
    "            label = rf'${attention_text}, {str(int(model))}$'\n",
    "            \n",
    "            ax.scatter(\n",
    "                xs, ys, s=size, c=curr_color, label=label, marker=marker,\n",
    "                linewidth=2, alpha=0.7, antialiased=True\n",
    "            )\n",
    "    \n",
    "    x_label_formatted = _build_axis_label(x_label, x_units)\n",
    "    y_label_formatted = _build_axis_label(y_label, y_units)\n",
    "    \n",
    "    ax.set_title(\n",
    "        r'$\\textbf{Tracker Performance Comparison --- ' + \n",
    "        rf'Training Iterations and Deformable Siamese Attention --- {x_label} vs. {y_label}}}$'\n",
    "    )\n",
    "    ax.set_xlabel(x_label_formatted)\n",
    "    ax.set_ylabel(y_label_formatted)\n",
    "    ax.legend(loc='best', markerscale=0.5, fontsize=14)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig_mota_motp = plot_tracker_attention_comparison(\n",
    "    df, 'mota', 'motp', 'MOTA', 'MOTP'\n",
    ")\n",
    "fig_mota_motp.savefig('tracker_cmp_plot_mota_motp.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_rec_prec = plot_tracker_attention_comparison(\n",
    "    df, 'recall', 'precision', 'Recall', 'Precision'\n",
    ")\n",
    "fig_rec_prec.savefig('tracker_cmp_plot_rec_prec.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76ca744f2f2bbdc513f12385c5c1408b7d19822a54ce48d4d56c79d4676f5a55"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('ml': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
